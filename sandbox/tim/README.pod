=head1 NAME

DBIT Prototype - testbed for exploring and experimenting with ideas

=head1 SYNOPSIS

Prerequisites:

    # Test::Database breaks with YAML::Tiny 1.58+ so pre-install a slightly older one
    # else you'll get "YAML::Tiny failed to classify line ..." warnings.
    # See https://rt.cpan.org/Ticket/Display.html?id=92916
    cpanm http://cpan.metacpan.org/authors/id/E/ET/ETHER/YAML-Tiny-1.56.tar.gz

    cpanm DBI Test::Database Class::Tiny # plus DBD::SQLite for example

Run:

    cd .../DBI-Test/sandbox/tim

    perl tumbler.pl && prove -r out

Once it's working for you, you can use this command to delete old output
directories and run the tests in parallel for much faster results:

    rm -rf out* && perl tumbler.pl && prove -j4 -sr out

Configure:

    perldoc Test::Database
    ... write a config file with dsn= entries ...
    (See tumbler.pl output for where that config file lives.)

=head1 DESCRIPTION

The tumbler.pl script scans @INC for 'test case modules' and writes a
directory tree of 'generated tests' which can be executed (in parallel and/or
shuffled) by prove.

The test case modules are in the DBI::TestCase namespace.
The out/ directory holds the generated tests.
If there's an existing old/ directory it's renamed with "-$epoch_time" appended.

The generated tests are thin wrappers around the test case modules which vary the
values of a number of contextual settings (e.g. environment variables) in a
range of combinations.

The contextual settings are divided into three groups: DBI, Driver, DBD.
Each group can generate a number of settings combinations for whatever the
current values of the previous group settings. Somewhat like a three-tumbler
combination lock.

=over 4

=item DBI

    DBI_PUREPERL
    DBI_AUTOPROXY

Typical combinations generated here are:

    Default        - no environment variables set
    pureperl       - DBI_PUREPERL=2
    gofer          - DBI_AUTOPROXY set to use DBD::Gofer
    pureperl_gofer - combines pureperl and gofer

plus extra variants for threads if your perl supports them.

=item Driver

    DBI_DRIVER

For each of the possible L</DBI> setting combinations generated above, multiple
variants of DBI_DRIVER will be generated, one for each available driver.

The values are those returned from the DBI->available_drivers() method, with
proxy drivers removed.  Also, if DBI_PUREPERL is set then non-pureperl drivers
are removed.

=item DBD

    DBI_DSN
    DBI_USER
    DBI_PASS

For each of the possible L</DBI> setting combinations and L</Driver> setting
combinations generated above, multiple variants of DBI_DSN etc. will be
generated.

The values for DBI_DSN, DBI_USER and DBI_PASS are derived from the handles
returned by L<Test::Database> via the call:

    my @handles = Test::Database->handles({ dbd => $DBI_DRIVER });

If there are no Test::Database C<dsn> configurations defined for a given DBI_DRIVER
then a warning will be generated and that driver will be skipped.

Note that Test::Database is able to generate test handles for some drivers,
like L<DBD::SQLite> and L<DBD::DBM>, automatically, without a config file.
TODO I suspect we're not cleaning up these properly.

Plugins (in future) might generate multiple variations for DBI_DSN by, for
example, adding attribute settings to test different modes of the driver.
For example, for DBI_DRIVER "DBM" the generated DBI_DSN variants might look like:

    "dbi::mldbm_type=...,dbm_type=..."

=back

Typically I<many> output tests will be generated for each input test.

DBD-level plugins may also arrange to include extra tests just for particular
drivers in particular configurations.


=head1 INTERNAL INFORMATION

=head2 Tumbler

TODO Needs a plugin mechanism for test variant generation, e.g. abstract out
dbd_dbm_settings_provider(). Note that plugins for *test variant* generation are
different to plugins for *test case* provision.  (The total set of generated
tests are "test variants" x "test cases".)

TODO Add support for more drivers to Test::Database (eg non-server DBDs).

=head2 Test Module Interface

The input tests are implemented as modules. Each generated test script simply
sets environment variables, loads the module, and then calls:

    $module_name->run_tests;

That is the entire interface.

The test module is free to implement that method in whatever way it likes.
The method is simply expected to generate standard test output using modules
like L<Test::More>.

=head2 Input Tests

Test case modules follow this naming convention:

    DBI::TestCase::(drh|dbh|sth)_(ro|rw)::...

Where C<{drh|dbh|sth}> is the type of handle the tests relate to,
and C<{ro|rw}> represents read-only, or read-write.

TODO we probably need a way to distinguish tests that do or don't modify handle
attributes after execution since that's an important limitation of DBD::Gofer.

For example:

    DBI::TestCase::dbh_ro::GetInfo
    DBI::TestCase::sth_ro::BasicPrepareExecuteSelect

TODO Start formulating a list of likely test files in order to help identify
what kind of fixtures will be needed, and to help define naming conventions.

TODO Work out how we namespace, distribute, package, install and use tests from
multiple sources.  Is reserving DBD::*::TestCase:: a good idea, or should test
provider plugins look after that? Consider version skew between DBIT and
third-party DBD tests.

=head2 Test Module Implementation

An experimental L<DBI::Test::CaseBase> class is provided to act as a base class for
test modules.  Test modules do not have to use it.

The DBI::Test::CaseBase class provides a C<run_tests> methods that instanciates an
object of that class and then calls a C<setup> method, a method which finds and calls
test methods, and finally a C<teardown> method.

TODO Currently method name introspection isn't implemented, so test modules
need to implement a get_subtest_method_names() method that just returns a list
of method names to be called. It's expected that a method naming convention
will be adopted to avoid the need for that.

The setup method connects to the database simply using DBI->connect()
with no arguments, which means that the environment variables set in the
generated test script are used.  The database handle is stored as an attribute
of the test object.

The setup method also instanciates a 'fixture provider' object (see below) for
the database handle and stores that as an attribute of the test object.

This arrangement makes it easy for a test module to run tests multiple times
with different arguments to the test object instanciation. For example:

    sub run_tests {
        $class->SUPER::run_tests( ...arguments... );
        $class->SUPER::run_tests( ...arguments... );
        $class->SUPER::run_tests( ...arguments... );
    }

The test methods are called on the test object instances.

If the current context doesn't support a fixture (see below) needed by the test
method then it can simply return. Other tests methods will still be called.

    sub get_subtest_method_names { qw(my_test_foo my_test_bar) }

    sub my_test_foo {
        my $self = shift; # the test object

        return if ...can't run this test in this context...;

        ...tests...
    }

    sub my_test_bar {
        my $self = shift; # the test object

        return if ...can't run this test in this context...;

        ...tests...
    }

=head3 Test Module Notes

Since we're not using subtest() (due to issues with threads) if you want to
call C<plan> you should only do so from the C<setup> method.

The teardown method calls done_testing() for you.

=head2 Fixture Provider

An experimental L<FixtureProvider> class is provided to act as an interface for
providing fixtures (e.g. database data and SQL statements) for tests to use.

The FixtureProvider class loads a corresponding C<FixtureProvider::$DBI_DRIVER>
and instanciates an instance of it, passing in the database handle.

The returned fixture provider object is expected to provide a number of methods
that return 'fixtures' (see below). Each method has a specific name, like
C<get_ro_stmt_select_1r2c_si>, and a corresponding definition of what the
expected behaviour of that fixture is. The definitions are quite strict
as they're effectively a contract between the fixture provider and the test.

If a driver can't support a given fixture definition in the current context
then the fixture provider for that driver should return undef. The caller will
then skip the tests that relied on that fixture.

    sub get_subtest_method_names { qw(basic_prepare_execute_select_ro) }

    sub basic_prepare_execute_select_ro {
        my $self = shift;

        my $fx = $self->fixture_provider->get_ro_stmt_select_1r2c_si;
        return warn "aborting: no get_ro_stmt_select_1r2c_si fixture"
            unless $fx;

        my $sth = $self->dbh->prepare($fx->statement);
        ...tests...
    }

Naturally tests should be written to use the simplest fixture that provides
sufficient functionality for what the test script is trying to test.

TODO Start making a list of fixtures needed to satisfy the needs of existing
kinds of tests. For example:

    errors (and warn & info)
    error state sharing between handles and propagation up to parent handle
    transactions
    unicode,
    FKs and other schema info methods
    param handling
    etc etc!

TODO Outline a way of describing the 'contract' supported by a
fixture i.e. what a fixture provider for a given driver has to honor and what a
test script can assume/expect.

TODO come up with a naming convention for fixture provider methods.

TODO Create more per-driver fixture provider subclasses (hopefully mostly empty).


=head2 Fixture

A fixture object is returned by a method call on a fixture provider object.
A typical fixture object has a C<statement> method that returns the statement
value to be passed to the DBI C<prepare> method.

When the fixture object was created by the fixture provider it may have created
a temporary table in the database and loaded data into it. If so the fixture
object will have a destructor that will drop the temporary table.

TODO Extend fixtures to support a sequence of statements?

TODO Find some way to let a fixture set $TODO in the callers package for Test::More?
Would be a handy way for a driver-specific fixture provider to express known-bugs
instead of being forced to return undef so loose the benefit of testing.
We'd probably need to mandate that the lifespan of fixture objects (per calling class)
should be non-overlapping.


=head1 Other TODOs

=head2 Encapsulate skip_all for an individual test method

I suspect that providing an equivalent of "plan skip_all => $msg" is probably sufficient.
skip_all throws a Test::Builder::Exception object, we could catch that.
We want to record what tests are skipped and why anyway as part of data recording.
(Note: we used to call the test methods via Test::More's subtest() but that
has issues when threads are enabled so was dropped.)

=head2 Data Recording

Consider what data we'd like to capture during a test run.
For example, what fixtures weren't provided (causing tests to be skipped) and
in which contexts that happened.

=cut
